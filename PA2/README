user: zhanghw
user: xyshen

Haowei Zhang: zhanghw
Xiaoying Shen: xyshen

README file for Programming Assignment 2 (C++ edition)
=====================================================

Your directory should now contain the following files:

 Makefile        -> [course dir]/src/PA2/Makefile
 README
 cool.flex
 test.cl
 lextest.cc      -> [course dir]/src/PA2/lextest.cc
 mycoolc         -> [course dir]/src/PA2/mycoolc
 stringtab.cc    -> [course dir]/src/PA2/stringtab.cc
 utilities.cc    -> [course dir]/src/PA2/utilities.cc
 handle_flags.cc -> [course dir]/src/PA2/handle_flags.cc
 *.d             dependency files
 *.*             other generated files

The include (.h) files for this assignment can be found in 
[course dir]/include/PA2

	The Makefile contains targets for compiling and running your
	program. DO NOT MODIFY.

	The README contains this info. Part of the assignment is to fill
	the README with the write-up for your project. You should
	explain design decisions, explain why your code is correct, and
	why your test cases are adequate. It is part of the assignment
	to clearly and concisely explain things in text as well as to
	comment your code. Just edit this file.

	cool.flex is a skeleton file for the specification of the
	lexical analyzer. You should complete it with your regular
	expressions, patterns and actions. 

	test.cl is a COOL program that you can test the lexical
	analyzer on. It contains some errors, so it won't compile with
	coolc. However, test.cl does not exercise all lexical
	constructs of COOL and part of your assignment is to rewrite
	test.cl with a complete set of tests for your lexical analyzer.

	cool-parse.h contains definitions that are used by almost all parts
	of the compiler. DO NOT MODIFY.

	stringtab.{cc|h} and stringtab_functions.h contains functions
        to manipulate the string tables.  DO NOT MODIFY.

	utilities.{cc|h} contains functions used by the main() part of
	the lextest program. You may want to use the strdup() function
	defined in here. Remember that you should not print anything
	from inside cool.flex! DO NOT MODIFY.

	lextest.cc contains the main function which will call your
	lexer and print out the tokens that it returns.  DO NOT MODIFY.

	mycoolc is a shell script that glues together the phases of the
	compiler using Unix pipes instead of statically linking code.  
	While inefficient, this architecture makes it easy to mix and match
	the components you write with those of the course compiler.
	DO NOT MODIFY.	

        cool-lexer.cc is the scanner generated by flex from cool.flex.
        DO NOT MODIFY IT, as your changes will be overritten the next
        time you run flex.

 	The *.d files are automatically generated Makefiles that capture
 	dependencies between source and header files in this directory.
 	These files are updated automatically by Makefile; see the gmake
 	documentation for a detailed explanation.

Instructions
------------

	To compile your lextest program type:

	% make lexer

	Run your lexer by putting your test input in a file 'foo.cl' and
	run the lextest program:

	% ./lexer foo.cl

	To run your lexer on the file test.cl type:

	% make dotest

	If you think your lexical analyzer is correct and behaves like
	the one we wrote, you can actually try 'mycoolc' and see whether
	it runs and produces correct code for any examples.
	If your lexical analyzer behaves in an
	unexpected manner, you may get errors anywhere, i.e. during
	parsing, during semantic analysis, during code generation or
	only when you run the produced code on spim. So beware.

	To turn in your work type:

	% make submit-clean

	And run the "submit" program following the instructions on the
	course web page.
	
	Running "submit" will collect the files cool.flex, test.cl,
	README, and test.output. Don't forget to edit the README file to
	include your write-up, and to write your own test cases in
	test.cl.

 	You may turn in the assignment as many times as you like.
	However, only the last version will be retained for
	grading.

	If you change architectures you must issue

	% make clean

	when you switch from one type of machine to the other.
	If at some point you get weird errors from the linker,	
	you probably forgot this step.

	GOOD LUCK!

---8<------8<------8<------8<---cut here---8<------8<------8<------8<---

Write-up for PA2
----------------
Design strategy:

1. In our design, besides INITIAL state, there are other 4 exclusive states: 
  dash_comments, star_comments, string, string_error.

    We need string error states, because lexical analyzer should resume after 
  the end of string. The analyzer should throws away any other character which
  matches, and it should know where to resume.

    However, although comments states could have errors, we do not need to add
  comment error states. Because error only happens when EOF happens in comment. 
  In this case, The analyzer will immediately return to the INITIAL state and
  return ERROR;

2. Priority to match. There are 5 basic lexeme types, Integer + ID + Special 
	Notation, strings, comments, keywords, and whitespaces. Only ID and keywords 
	share same characters. So it is easy to differentiate other states from ID 
	and keywords.

3. For ID and keywords, keywords should have high priority, so keywords 
  matching should go first than ID. Type ID, Object ID and integers are 
	differentiated with initial character. 

4. For white space, \n needs to be processed specially, because it increases 
	line number.

5. From the INITIAL state, whether "--" or "(*" matches first, the state goes 
	to dash_comment state or star_coment state. In each state, \n increases line 
	number. In star_comment state, regular expression not begin with "*", "(" 
	will be eaten up. Regular expression begin with "*"+")", "("+"*" tells the 
	analyzer to exit or enter one level of nested comments. comments_nest_level 
	indicates which comment level the analyzer is reading. 

		In our design, in dash_comments or star_comments state, the matching rules 
	covers all possible regular expressions.

6. Handle Null Character in string and string length exceeds the maximum limit.
	In those two cases, the analyzer will enter string_error state (except the 
	string end with \", but \0 could not be written into the buffer. Now the 
	analyzer will resume from INITIAL state). To capture Max length error, before
	every time the string buffer is written, the analyzer will check whether the 
	pointer will pass the buffer boundary. When analyzer is in the string error 
	state, it will exit from this state after matching \n, \" or EOF.

7. Handle back slash and escape line case. In string, back slash is specially 
	handled. <string>\\[ntbf] will match the same number of characters compared 
	with <string>\\[^\n\0], but since \n\t\b\f is specially handled, these rules 
	need to be placed to have higher priority. 
	
		All possible regular expressions under string condition are matched. The 
	sub-string begins with back slash (\), null, \n, and \" is has special rules.

8. EOF may happen in all states, which needs to be processed separately. In 
  string_error state, error message is not re-generated.

-----------------END OF DESIGN STRATEGY-------------------------<<<<<<<


-----------------TEST STRATEGY AND TEST CASE DESCRIPTION----------------->>>>>

We use test.cl to have basic test, including comments, keywords, ID, INT_CONST,
White space, string, invalid character and EOF in string.

All other test cases are in sub directory tests/

Our specially designed Test cases in sub directory tests/ is decribed 
following, including:

++++ Keywords
++++ Comments Test
++++ String Test
++++ All Characters Test

++++ Keywords:
2.cl and tests/2_true_false.cl: For keywords not covered in test.cl, we 
	have our own test cases. We also test case insensitive for keywords.

++++ Comments Test:
1.cl: First test nested comments, test "("+.*, "("+\n, "*"+.*, "*"+\n, and \n. 
	This tests tries to cover all possible regular expression under star_comment
	condition; Then basic tests for some key words, type id and object id
		
6_comments.cl: Test nested comments. Test unmatched "*)" and then matched "*)".
	Test dash_comments and star_comments. Test EOF in comments
-----------------------------------

++++ String Test:

>> Test string with \" match and mismatch

5_string_mismatch.cl: Test string with \" match and mismatch (with out \" or an
	unescaped new line in string)
	
	<<

>> These tests test string with a NULL character:

5_string_null.cl: Test a string with a null character. Test whether lexer could
	retume. Test string with a NULL character, end then EOF. Test whether lexer
	could get out from string_error state when meeting EOF.

	<<
	
>> These tests test string length:

3_too_long_string.cl: Test a very long string. Then test if lexer could resume

16_long_string_EOF: Test a string with 1025 characters then EOF

17_long_string.cl: Test 1025 characters string, "String constant too long"
	message should be shown. Test 1024 characters string with an escape line. 
	This String is still too long because \n occupy one byte. Test 1023 
	characters string with "\t""\n". Total is still 1025 characters. Test a 
	string with \b\t\f\n and other character c, \c. Test a normal string, 1024
	characters in total length, this is a string with valid length. Finally test 
	a 1025 character string and EOF 
	within this string.
		
		This test cases tries to catch edge cases, which is string length just one
	character longer or shorter than the max sting length.

	<<
	
>>Test sub-string begin with "\":

11_string_escape_line.cl: Test string with an escaped new line "\"\n

12_string_escape_null.cl: Test string with "\"[\0] and "\0"

14_string_escape_quote.cl: Test string with "\"[\"]

15_string_back_slash.cl: Try to catch all possible sub strings begins with "\"

	<<

>> Test EOF happens in a string:

7_EOF_string.cl and 8_EOF_string.cl: test EOF in a string. This string is a 
	normal string
	
Now our test cases catches all possible valid and invalid strings.
-------------------------------------

++++ Test all characters
20_all_character.cl: We use perl to generate any one or two combination of 
	ASCII char to test. This catches all invalid characters. We compared our 
	lexer and the reference lexer. The result is the same.
	
-------------------------------------
